---
title: '0.1 + 0.2 = 0.30000000000000004'
date: 2020-09-14
permalink: /posts/2020/09/0.30000000000000004/
tags:
  - floating-point
---
If you are newbie or experienced coder, I deeply believe you ever encounter a scenario that you do some arithmetic computation on decimal number like:
 `System.out.println(0.1 + 0.2)` and surprisingly, the returned value isn't as expected. You've got 0.30000000000000004. 

However `System.out.println(0.1 + 0.3)` show on my screen `0.4`. 
At the first time, I was thought my computer is lag. It's hard to believe that a modern computer can't produce the result exactly as my old hand calculator can do.
I tried to look up on Google, but I couldn't found any available answer succinct enough to understand. I was daunted. My brain told me: "Let's give up, buddy, we're good without this knowledge". 
As a result, I hardly ever think about this question until someday I found out the way the computer work with the floating-point number. 
If you had been wondering why this magical equation happened like me, this is a time to unveil the behind mechanism's secret.

The first, we have to know that computer doesn't know anything like `0` and `1`. Two number is symbolic number represent for two possible electrical signal state(on/off) of the transistor.
Because of that, the computer must represent everything in the form of `0s` and `1s`. It's look different from our standard decimal system which is include ten number from zero to nine. 
So, as you probably know, when you entered a decimal number, the compiler/interpreter have to convert to binary's digit.

To ensure all computers yield the same result when compute with floating-point number, the `IEEE Standard for Floating-Point Arithmetic (IEEE 754)` come into exists using the idea of scientific notation.
You just factor your number into two parts: a value whose magnitude is in the range of 1 â‰¤ n < 2, and a power of 2 number. For example:
            
$0.1 \quad \textrm{ is written as }  \quad 1.6 \times 2^{-4}$          
$0.2 \quad \textrm{ is written as }  \quad 1.6 \times 2^{-3}$      

It's pretty easy to represent integers as binary number using two's complement but in order to convert the floating-point number to binary number, the first step we need convert the floating-point number to the following form (scientific notation): 
$(-1)^{\color{purple}{\textrm{sign bit}}} (1 + \color{red}{\textrm{fraction}})  \times 2^{\textrm{$\color{green}{\textrm{exponent}}$ - bias}}$

![](https://s3-ap-southeast-1.amazonaws.com/logbasex.github.io/images/618px-IEEE_754_Double_Floating_Point_Format.svg.png)